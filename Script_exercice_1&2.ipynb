{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Groover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.compat.v1.losses import mean_squared_error\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "Type_influencer=[\n",
    "        'Media',          \n",
    "        'Radio',           \n",
    "        'Label',           \n",
    "        'Playlist',        \n",
    "        'Journalist',      \n",
    "        'Channel',         \n",
    "        'Booker',          \n",
    "        'Mentor',          \n",
    "        'Manager',         \n",
    "        'Springboard',     \n",
    "        'Publisher',       \n",
    "        'Supervisor',\n",
    "        'Event'  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### préparation des données pour l'algorithme Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_DF(DF):\n",
    "    DF_help=DF.apply(func = lambda S:S.lower())\n",
    "    DF_help=remplacer_site_DF(DF)\n",
    "    DF_help=DF_help.apply(func = lambda S:S.strip())\n",
    "    #DF_help=espacer_point_DF(DF)\n",
    "    return DF_help\n",
    "\n",
    "def get_longest_feedback(List):\n",
    "    return max([len(l) for l in List])\n",
    "\n",
    "def prepare_sentences_List(DF):\n",
    "    List=[sentence for paragraph in DF.values.tolist() for sentence in paragraph]\n",
    "    return List\n",
    "\n",
    "def prepare_sentences_help(DF):\n",
    "    DF_help = DF.apply(func = lambda S:[s.split(' ') for s in S.split('.')])\n",
    "    DF_help = DF_help.apply(func = lambda paragraphe : [ [word for word in sentence if not(word in [' ',''])]for sentence in paragraphe ])\n",
    "    DF_help = DF_help.apply(func = lambda paragraphe : [ sentence for sentence in paragraphe if not(sentence in [[' '],['']] or len(sentence)==0) ])\n",
    "    \n",
    "    #DF_help=DF_help.apply(func=lambda paragraph :[ sentence[1:] if sentence[0]=='' else sentence for sentence in paragraph])\n",
    "    return DF_help\n",
    "\n",
    "def remplacer_site(String):       \n",
    "    for index in [m.start() for m in re.finditer('.', String)]:\n",
    "        if String[:index].rfind('http')>String[:index].rfind(' '):\n",
    "            String=String[:index]+'&'+String[index+1:]\n",
    "    return String\n",
    "\n",
    "def remplacer_site_DF(DF):\n",
    "    DF=DF.apply(func= lambda S : remplacer_site(S))\n",
    "    return DF       \n",
    "\n",
    "def espacer_point_DF(DF):\n",
    "    DF=DF.apply(func = lambda S:S.replace('.',' .'))\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un dictionnaire Word2Vec, et encodage du feedbacks sous forme de de vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(sentences,window,size):\n",
    "    model=Word2Vec(sentences, size=size, window=window, min_count=1)\n",
    "    #,negative=1,hs=1\n",
    "    return model\n",
    "  \n",
    "def w2v(DF, dico, size_embeding, size_sentence):\n",
    "    DF=DF.apply(func = lambda String : np.array([dico[word] for word in String.split(' ')]+[np.zeros(size_embeding) for i in range (size_sentence-len(String.split(' ')))]))\n",
    "    return DF\n",
    "\"\"\"\n",
    "cette fonction encode les paragraphe de la colonne feedback sous la forme d'une suite de vecteurs encodant chacun une phrase,\n",
    "l'encodage d'une phrase est obtenue grâce à la moyenne des encodages des motes composant la phrase. L'encodage pourrait être \n",
    "ameilloré en prenant en compte la probabilité qu'un mot soit inclu dans un phrase, plus cette dernière serait faible plus la\n",
    "pondérantion du mot serait élevée.\n",
    "\"\"\"\n",
    "def sentence_2v_bis(DF,dico,size_embeding, size_paragraph):\n",
    "    DF_bis=pd.Series([None for i in range (DF.shape[0])])\n",
    "    DF_bis.index=DF.index\n",
    "    for i in range (DF.shape[0]):\n",
    "        DF_bis.iloc[i]=np.stack([np.average([dico.get_vector(word) for word in sentence], axis=0) for sentence in  DF.iloc[i]]+[np.zeros(size_embeding) for i in range (size_paragraph-len(DF.iloc[i]))],axis=0)\n",
    "    return DF_bis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construit un modèle d'analyse de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model( size_encoding, max_legnth_para, numClasses, lstmUnits):\n",
    "\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(LSTM(120,input_shape=(max_legnth_para,size_encoding), activation='relu', return_sequences=True ))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(120,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(numClasses,activation='sigmoid'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "    model.compile(\n",
    "                loss='mean_squared_error',\n",
    "                optimizer=opt,\n",
    "                metrics=['mean_absolute_error'],)\n",
    "    model=model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle encodant les artistes sous forme de vecteurs et les influenceurs sous forme de matrice, ce modèle peut être entrainé grâce à la base de donnée fournie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class feature_extractor ():\n",
    "    \n",
    "    def __init__(self, Data,N_features):\n",
    "        \n",
    "        output_data=Data.drop(['influencer_id','acceptation_rate','band_id','date_created']+Type_influencer, axis=1)\n",
    "        number_output=output_data.shape[1]\n",
    "        \n",
    "        influencer_info=Data[['influencer_id','acceptation_rate']+Type_influencer]\n",
    "        influencer_info=influencer_info.groupby('influencer_id').tail(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        band_ids=pd.DataFrame({'band_id':pd.unique(Data['band_id']).tolist()})\n",
    "        influencer_ids=pd.DataFrame({'influencer_id':pd.unique(Data['influencer_id']).tolist()})\n",
    "        \n",
    "        Number_influencer=influencer_ids.shape[0]\n",
    "        Number_band=band_ids.shape[0]\n",
    "        \n",
    "        band_ids['index_of_id']=pd.Series([i for i in range(Number_band)])\n",
    "        band_ids=band_ids.set_index('band_id')\n",
    "        influencer_ids['index_of_id']=pd.Series([i for i in range(Number_influencer)])\n",
    "        influencer_ids=influencer_ids.set_index('influencer_id')\n",
    "        \n",
    "        couple=Data[['band_id','influencer_id']]\n",
    "        couple_index=pd.DataFrame(index=couple.index , columns= ['band_index','influencer_index'])\n",
    "        couple_index['band_index']=couple['band_id'].apply(func = lambda R : band_ids.loc[R])\n",
    "        couple_index['influencer_index']=couple['influencer_id'].apply(func = lambda R : influencer_ids.loc[R])\n",
    "        \n",
    "        \n",
    "        band_features=tf.Variable(tf.random.normal([Number_band, N_features+15]))\n",
    "        \n",
    "        influencer_info_T=tf.constant(np.repeat(influencer_info.values[np.newaxis,...], [number_output], axis=0), dtype=float)\n",
    "        influencer_features_help=tf.Variable(tf.random.normal([number_output,Number_influencer,N_features]))\n",
    "        influencer_features=tf.concat([influencer_info_T,influencer_features_help], axis=2)\n",
    "        influencer_features=tf.transpose(influencer_features,[0,2,1])\n",
    "        \n",
    "        print(influencer_features.shape)\n",
    "        potential_pred=tf.linalg.matmul(band_features, influencer_features,)\n",
    "        potential_pred=tf.transpose(potential_pred,perm=[1,2,0])\n",
    "        pred_vec=tf.gather_nd(potential_pred, list(zip(couple_index['band_index'].tolist(),couple_index['influencer_index'].tolist())))\n",
    "        \n",
    "        output_T=tf.constant(output_data.astype(float).values)\n",
    "        \n",
    "        loss=mean_squared_error( output_T, pred_vec)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.influencer_features=influencer_features_help\n",
    "        self.band_features=band_features\n",
    "        self.loss=loss\n",
    "        self.band_features=band_features\n",
    "        self.influencer_features=influencer_features\n",
    "        self.band_ids=band_ids\n",
    "        self.influencer_ids=influencer_ids\n",
    "    \n",
    "    def optimiser(self,epochs):\n",
    "        sess = tf.compat.v1.Session()\n",
    "        opt=tf.train.AdamOptimizer(1e-3).minimize(self.loss)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range (epochs) :\n",
    "            sess.run(opt)\n",
    "            \n",
    "        sess.close()\n",
    "        \n",
    "    def get_features(self):\n",
    "        sess = tf.compat.v1.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        influ_feat=sess.run(self.influencer_features)\n",
    "        band_feat=sess.run(self.influencer_features)\n",
    "        influ_feat=[influ_feat[i,:,:]for i in range(influ_feat.shape[0])]\n",
    "        band_feat=[band_feat[i,:]for i in range(band_feat.shape[0])]\n",
    "        influ_feat=pd.DataFrame(index=self.influencer_ids.values,data={'feat':influ_feat})\n",
    "        band_feat=pd.DataFrame(index=self.band_ids.values, data={'feat':band_feat,'id_s':self.band_ids})\n",
    "        \n",
    "        band_feat.\n",
    "        return influ_feat,band_feat\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prétraitement de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logan\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py:931: UserWarning: Ignoring URL 'https://groover.co/media/components/sendButton/sendIcon.pngDu%20bon%20ensemble%20musical%20que%20n'aurait%20pas%20renié%20Santana%20notamment%20sur%20le%20titre%20%22distance%22%20(super%20guitare).%20Je%20trouve%20cependant%20l'humeur%20des%20morceaux%20un%20peu%20légère,%20cela%20repose%20la%20question%20de%20la%20destination%20d'un%20tel%20style%20musical.%20A%20savoir%20ambiancer%20une%20soirée%20dans%20un%20lieu%20commun,%20restaurant%20au%20bord%20du%20lac,%20réception%20?%20Ou%20alors%20pouvoir%20l'écouter%20chez-soi%20de%20manière%20plus%20contemplative%20?%20Ainsi%20la%20résonance%20de%20votre%20musique%20ne%20m'amène%20pas%20plus%20loin%20qu'une%20soirée%20dans%20une%20marina,%20je%20ne%20suis%20pas%20parvenu%20à%20m'extraire%20de%20cette%20dimension%20fonctionnaliste%20que%20votre%20musique%20stimule...%20J'espère%20que%20ce%20retour%20vous%20sera%20utile%20d'une%20manière%20ou%20d'une%20autre,%20votre%20travail%20est%20de%20qualité,%20là%20n'est%20pas%20la%20question.%20Bonne%20continuation%20!' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n"
     ]
    }
   ],
   "source": [
    "DF=pd.read_csv('groover_dataset_challenge.csv', index_col = 0)\n",
    "    \n",
    "    \n",
    "DF['feedback']=DF['feedback'].fillna(value='')\n",
    "\n",
    "\n",
    "DF['decision']=DF['decision'].apply(func = lambda R : np.array(re.sub('[\\]\\[\\']', '', R).split(',')))\n",
    "\n",
    "\n",
    "#!!!! coprendre ce qu'on fait la\n",
    "DF=DF.drop('decision', axis = 1).join(DF.decision.str.join('|').str.get_dummies())\n",
    "DF=DF.drop('influencer_kind', axis = 1).join(pd.get_dummies(DF['influencer_kind']))\n",
    "\n",
    "DF['acceptation_rate']=DF['acceptation_rate'].fillna(0)\n",
    "\n",
    "DF['feedback']=DF['feedback'].apply(func = lambda R : R.replace('\\n',''))\n",
    "\n",
    "DF.to_excel(\"processed_DB.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chargement de la base de donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=pd.read_excel('processed_DB.xlsx', index_col = 0)\n",
    "DF['feedback']=DF['feedback'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### création d'un dictionnaire word ->vec et d'un vecteur contenant les encodages des feedback sous forme de vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=2\n",
    "embeding_size=100\n",
    "num_Lstm=120\n",
    "\n",
    "DF_sentiment=DF.drop(['track__id','band_id','influencer_id','date_created','acceptation_rate']+Type_influencer, axis=1)\n",
    "feedbacks=DF_sentiment['feedback']\n",
    "\n",
    "feedbacks=prepare_DF(feedbacks)\n",
    "feedbacks=prepare_sentences_help(feedbacks)\n",
    "List=prepare_sentences_List(feedbacks)\n",
    "\n",
    "num_class=1\n",
    "Max_size_paragraph=max(feedbacks.apply(func = lambda L : len(L)))\n",
    "\n",
    "dico=generate_dict(List, window_size, embeding_size)\n",
    "Dico=dico.wv\n",
    "\n",
    "path = get_tmpfile(\"wordvectors.kv\")\n",
    "Dico.save(path)\n",
    "\n",
    "\"\"\"\n",
    "On encoder les paragraphe de la colonne feedback sous la forme d'une suite de vecteurs encodant chacun une phrase,\n",
    "l'encodage d'une phrase est obtenue grâce à la moyenne des encodages des motes composant la phrase. L'encodage pourrait être \n",
    "ameilloré en prenant en compte la probabilité qu'un mot soit inclu dans un phrase, plus cette dernière serait faible plus la\n",
    "pondérantion du mot serait élevée.\n",
    "\"\"\"\n",
    "feedbacks_vec=sentence_2v_bis(feedbacks,Dico,embeding_size,Max_size_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### création et entrainement d'un modèle d'analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\logan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\logan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/3\n",
      "70346/70346 [==============================] - 127s 2ms/sample - loss: 0.2770 - mean_absolute_error: 0.3527\n",
      "Epoch 2/3\n",
      "70346/70346 [==============================] - 130s 2ms/sample - loss: 0.6761 - mean_absolute_error: 0.7065s - loss: 0.6762 - \n",
      "Epoch 3/3\n",
      "70346/70346 [==============================] - 127s 2ms/sample - loss: 0.6751 - mean_absolute_error: 0.7055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nCe morceau de code permet de mesurer l'éfficacité du modèle avec un set de test, on pourrait utiliser de la cross validation\\navoir une vision plus globale de l'efficacité du modèle\\n\\nData_Train=Data.iloc[:50000]\\nData_Test=Data.iloc[50000:]\\nXtrain=np.stack(Data_Train['feedback'].tolist(),axis=0)\\nYtrain=np.stack(Data_Train['score'].tolist(),axis=0)\\nXtest=np.stack(Data_Test['feedback'].tolist(),axis=0)\\nYtest=np.stack(Data_Test['score'].tolist(),axis=0)\\nModel=build_model(embeding_size, Max_size_paragraph, num_class, num_Lstm)\\nModel.fit(Xtrain,Ytrain , epochs=3, validation_data=(Xtest,Ytest))\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_sentiment=DF_sentiment\n",
    "Data_sentiment['feedback']=feedbacks_vec\n",
    "\n",
    "Xtrain=np.stack(Data_sentiment['feedback'].tolist(),axis=0)\n",
    "Ytrain=np.stack(Data_sentiment['score'].tolist(),axis=0)\n",
    "Model=build_model(embeding_size, Max_size_paragraph, num_class, num_Lstm)\n",
    "Model.fit(Xtrain,Ytrain , epochs=3)\n",
    "\n",
    "\"\"\"\n",
    "Ce morceau de code permet de mesurer l'éfficacité du modèle avec un set de test, on pourrait utiliser de la cross validation\n",
    "avoir une vision plus globale de l'efficacité du modèle\n",
    "\n",
    "Data_Train=Data.iloc[:50000]\n",
    "Data_Test=Data.iloc[50000:]\n",
    "Xtrain=np.stack(Data_Train['feedback'].tolist(),axis=0)\n",
    "Ytrain=np.stack(Data_Train['score'].tolist(),axis=0)\n",
    "Xtest=np.stack(Data_Test['feedback'].tolist(),axis=0)\n",
    "Ytest=np.stack(Data_Test['score'].tolist(),axis=0)\n",
    "Model=build_model(embeding_size, Max_size_paragraph, num_class, num_Lstm)\n",
    "Model.fit(Xtrain,Ytrain , epochs=3, validation_data=(Xtest,Ytest))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage du feedback grâce aux résultats du modèle d'analyse de sentiments et entrainement des vecteurs représentant les artistes et les influenceurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 25, 554)\n"
     ]
    }
   ],
   "source": [
    "Data_matching=DF.drop('date_created', axis=1)\n",
    "Data_matching['feedback']=feedbacks_vec\n",
    "Data_matching['feedback']=Data_matching['feedback'].apply(func= lambda feedback : Model.predict(feedback[np.newaxis,...]))\n",
    "\n",
    "#DF_copy['feedback']=feedbacks_vec.apply(func = lambda R:Model.predict(R))\n",
    "\n",
    "fe=feature_extractor(Data_matching,10)\n",
    "fe.optimiser(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "influ_feat,band_feat=fe.get_features()\n",
    "feat_band_6593=band_feat.loc[6593]\n",
    "Distance_6593=band_feat.apply(func = lambda F:numpy.linalg.norm(F-feat_band_6593))\n",
    "10_smallest_dit=Distance_6593.nsmallest(10,'feat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
